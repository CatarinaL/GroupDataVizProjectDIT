{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization Project\n",
    "\n",
    "## Airplane Crashes Since 1908\n",
    "\n",
    "***\n",
    "\n",
    "Authors **Catarina Louren√ßo**\n",
    "\n",
    "Student Numbers **C17709355**\n",
    "\n",
    "Module **Big Data Concepts**\n",
    "\n",
    "Lecturer **Abhishek Kaushik**\n",
    "\n",
    "Course-Year **DT302-2 Business Analytics**\n",
    "\n",
    "Submitted **01-04-2019**\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Data Visualization Project using Python and other tools, exploring [open data about aviation crashes](https://www.kaggle.com/saurograndi/airplane-crashes-since-1908), sourced from Kaggle.\n",
    "\n",
    "*Python installation version:* anaconda3-5.1.0\n",
    "\n",
    "*Additional packages:* geopy\n",
    "\n",
    "*Other tools:* Google Fusion Tables, Excel\n",
    "\n",
    "\n",
    "### Open data source selected: \n",
    "[Aviation crashes dataset](https://www.kaggle.com/saurograndi/airplane-crashes-since-1908)\n",
    "\n",
    "#### Database Format\n",
    "| Column Name | Summary |\n",
    "|---------------|------------------------------------------------------|\n",
    "| Date:         | Date of accident,  in the format - January 01, 2001 |\n",
    "| Time:         | Local time, in 24 hr. format unless otherwise specified |\n",
    "| Airline/Op:   | Airline or operator of the aircraft |\n",
    "| Flight #:     | Flight number assigned by the aircraft operator |\n",
    "| Route:        | Complete or partial route flown prior to the accident |\n",
    "| AC Type:      | Aircraft type |\n",
    "| Reg:          | ICAO registration of the aircraft |\n",
    "| cn / ln:      | Construction or serial number / Line or fuselage number |\n",
    "| Aboard:\t    | Total aboard (passengers / crew) |\n",
    "| Fatalities:\t| Total fatalities aboard (passengers / crew) |\n",
    "| Ground:\t    | Total killed on the ground |\n",
    "| Summary:\t    | Brief description of the accident and cause if known |\n",
    "\n",
    "### Final dataset used for analysis \n",
    "\n",
    "The original dataset wasn't used *as-is*, as we had to change some of its structure and contents in order to prepare it for the kind of analysis we had proposed ourselves to perform. \n",
    "\n",
    "**Data cleaning** involved mainly enforcing a coherent format within each column, decluttering text from words like \"near\" to keep only the relevant information, replacing NaN and strings by the correct numeric literal.\n",
    "\n",
    "**Additions** Two columns for GPS coordinates, Latitude and Longitude. This was done using the geopy library (explain further in another section...) to scrap data from BingMaps. The script used can be found [here](add_coordinates.py).\n",
    "\n",
    "**Deletions** The columns *Flight #*, *Route*, *Registration*, *cn/In* were dropped as we didn't require them for our analysis. Additionally, three rows for which the entries couldn't be verified were also dropped.\n",
    "\n",
    "[This is the dataset](dataset/clean_v1_Airplane_Crashes_and_Fatalities_Since_1908.csv) that we used to start cleaning and preparing the data using pandas - See Part 1. Data Preparation.\n",
    "\n",
    "[This is the final dataset](dataset/dataset_final.csv) that we used to proceed with analysis - See Part 2. Exploratory Data Analysis.\n",
    "\n",
    "### Questions to be answered: \n",
    "\n",
    "**Descriptive:**\n",
    "* Fatalities before and after hitting the ground\n",
    "* Where do most planes crash (by time period)? Heat map with locations - period of 5-10 years? - change over time - animation/several time periods (WW2)?\n",
    "* Separate into sections? Location by continent, night/day flight, type of aircraft etc\n",
    "* Is there a specific make/model that appears more frequently than others\n",
    "* Are they mainly passenger airplanes/cargo airplanes\n",
    "* Does the age of the plane matter,ie, an older/newer plane regarding crashes\n",
    "\n",
    "**Predictive:**\n",
    "* When/where should we expect the next crash to happen? With how many fatalities?\n",
    "\n",
    "**Text analysis:**\n",
    "* Text analysis: find causes in description text\n",
    "* (Advanced) Text analysis - sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "---\n",
    "Cleaning, completing, dropping, converting\n",
    "\n",
    "### 1.1 Inspecting the Data\n",
    "\n",
    "After the initial data read and conversion to a dataframe, we were able to run the ```info()``` method, which showed us there were several missing values - we are able to tell this by the count of values on each column against the total of 5268 entries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5268 entries, 0 to 5267\n",
      "Data columns (total 11 columns):\n",
      "Date          5268 non-null object\n",
      "Time          3049 non-null object\n",
      "Location      5248 non-null object\n",
      "Latitude      5240 non-null float64\n",
      "Longitude     5240 non-null float64\n",
      "Operator      5250 non-null object\n",
      "Type          5241 non-null object\n",
      "Aboard        5246 non-null float64\n",
      "Fatalities    5256 non-null float64\n",
      "Ground        5246 non-null float64\n",
      "Summary       4878 non-null object\n",
      "dtypes: float64(5), object(6)\n",
      "memory usage: 452.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"dataset/clean_v1_Airplane_Crashes_and_Fatalities_Since_1908.csv\"\n",
    "dframe = pd.read_csv(file_path)\n",
    "\n",
    "# get information about data entries by column:\n",
    "dframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Date          5268 \n",
    "* Time          3049  *-> missing 2219 values*\n",
    "* Location      5248  *-> missing 20 values*\n",
    "* Latitude      5240  *-> missing 28 values (generated from Location, missing 8)*\n",
    "* Longitude     5240  *-> missing 28 values (generated from Location, missing 8)*\n",
    "* Operator      5250  *-> missing 18 values*\n",
    "* Type          5241  *-> missing 19 values*\n",
    "* Aboard        5246  *-> missing 22 values*\n",
    "* Fatalities    5256  *-> missing 12 values*\n",
    "* Ground        5246  *-> missing 22 values*\n",
    "* Summary       4878  *-> missing 390 values*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Aboard</th>\n",
       "      <th>Fatalities</th>\n",
       "      <th>Ground</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5240.000000</td>\n",
       "      <td>5240.000000</td>\n",
       "      <td>5246.000000</td>\n",
       "      <td>5256.000000</td>\n",
       "      <td>5246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.010367</td>\n",
       "      <td>-16.061076</td>\n",
       "      <td>27.554518</td>\n",
       "      <td>20.068303</td>\n",
       "      <td>1.608845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.570333</td>\n",
       "      <td>83.419121</td>\n",
       "      <td>43.076711</td>\n",
       "      <td>33.199952</td>\n",
       "      <td>53.987827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-77.529716</td>\n",
       "      <td>-176.669861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.944428</td>\n",
       "      <td>-82.810095</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>34.392923</td>\n",
       "      <td>-9.149480</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.801488</td>\n",
       "      <td>37.615021</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.449997</td>\n",
       "      <td>178.800476</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>2750.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Latitude    Longitude       Aboard   Fatalities       Ground\n",
       "count  5240.000000  5240.000000  5246.000000  5256.000000  5246.000000\n",
       "mean     27.010367   -16.061076    27.554518    20.068303     1.608845\n",
       "std      24.570333    83.419121    43.076711    33.199952    53.987827\n",
       "min     -77.529716  -176.669861     0.000000     0.000000     0.000000\n",
       "25%      10.944428   -82.810095     5.000000     3.000000     0.000000\n",
       "50%      34.392923    -9.149480    13.000000     9.000000     0.000000\n",
       "75%      43.801488    37.615021    30.000000    23.000000     0.000000\n",
       "max      80.449997   178.800476   644.000000   583.000000  2750.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the ```describe()``` method, we can spot some possible errors in Aboard: the minimum shouldn't be zero, because there are no unmanned flights in this dataset - we'll have to investigate this, as it could be due to missing values or a wrong value. Ground also has a maximum value that is abnormaly high - 2750 people killed on impact. All these warrant inspection.\n",
    "\n",
    "The fact that there are only 4673 unique summaries of the crash doesn't necessarily point to duplicate entries - it might just be the summary was the same kind of description, such as \"Crashed on landing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Cleaning\n",
    "\n",
    "Before proceeding with inspecting individual rows, changes to datatypes or removing duplicates, we checked for NaN and null values.\n",
    "\n",
    "> isna(obj)\t    Detect missing values for an array-like object.\n",
    ">\n",
    "> isnull(obj)\tDetect missing values for an array-like object.\n",
    ">\n",
    "> notna(obj)\tDetect non-missing values for an array-like object.\n",
    ">\n",
    "> notnull(obj)\tDetect non-missing values for an array-like object.\n",
    "\n",
    "For that purpose, we created two small methods that check for these column-wise:\n",
    "\n",
    "```python\n",
    "#Nan and lower than x values (useful for checking zeros, for instance)\n",
    "\n",
    "def check_nulls(column, dataframe=dframe):\n",
    "    column_series = dataframe[column]\n",
    "    null_column = column_series[(column_series.isna()) | (column_series.isnull())]\n",
    "    print(\"No null values in {0} series.\".format(column)) if null_column.empty else print(null_column)\n",
    "\n",
    "def check_lower_than(column, value=1, dataframe=dframe):\n",
    "    column_series = dataframe[column]\n",
    "    filtered_column = column_series[(column_series < value)]\n",
    "    print(\"No values lower than {1} in {0} series.\".format(column, value)) if filtered_column.empty else print(filtered_column)\n",
    "\n",
    "```\n",
    "\n",
    "Using these, we could generate small reports that tell us the rows we need to fix (NaN), and possibly investigate to check if the values are correct (zeros). A ```0``` on the Ground column is not \"supicious\", in fact it is the most common value, as that column indicates fatalities *on impact*. However, a ```0``` on Aboard is definitely a mistake.\n",
    "\n",
    "***\n",
    "An example:\n",
    "\n",
    ">--- ABOARD\n",
    ">\n",
    ">No null values in Aboard series.\n",
    ">\n",
    ">\n",
    ">*3307*    0\n",
    ">\n",
    ">*3611*   0\n",
    ">\n",
    ">Name: Aboard, dtype: int16\n",
    "\n",
    "***\n",
    "\n",
    "The process of filling these missing values was slow and laborious, as each entry had to be checked, researched and changed manually. This is documented in detail in the Data Preparation notebook (DataPreparation.ipynb) in annex to this report.\n",
    "\n",
    "During this process, the fact that many of the entries contained the wrong information became apparent - the wrong date, misspellings, wrong values for Aboard or Fatalities. Nothing too egregious, but it was a reminder that this data was scrapped by someone else, from a source that someone else had put together online, and so its veracity isn't exactly 100%.\n",
    "\n",
    "Another type of cleaning we performed was on Summary and Location text. For Summaries, we filled empty strings or null values with a placeholder that reads \"No details available\". For Locations, we identified a few recurring words and typos and corrected them using the ```.str.replace()``` method that pandas has available for ```DataFrame``` objects.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "#check if these words are in the values\n",
    "df[df.Location.str.contains(r\"Near|Off the|Over the\")][\"Location\"]\n",
    "\n",
    "#to remove/replace those words and then strip trailing whitespaces\n",
    "df.Location = df.Location.str.replace(\"Near\", \"\")\n",
    "df.Location = df.Location.str.replace(\"Over the\", \"\")\n",
    "df.Location = df.Location.str.strip()\n",
    "#replace \n",
    "df.Location = df.Location.str.replace(\"iO\", \"ic O\")\n",
    "df.Location = df.Location.str.replace(\"iR\", \"ic R\")\n",
    "df.Location = df.Location.str.replace(\"iG\", \"ic G\")\n",
    "\n",
    "#check\n",
    "df[df.Location.str.contains(r\"i[\\\\A-Z]\")][\"Location\"] #should return empty list if all these typos were corrected\n",
    "\n",
    "```\n",
    "\n",
    "#### 1.2.1 Adding geographic data with GeoPy\n",
    "\n",
    "The final step in data completion was to run the ```add_coordinates.py``` script on a .csv file with all the location names cleaned up, which returned a second .csv file with the appended data for latitude and longitude.\n",
    "For this script, we used the GeoPy library to handle the API requests to a Geolocation service of our choice - for ease of use and accuracy of results, Bing Maps was selected. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Type conversions\n",
    "\n",
    "With all the values in place, we could then proceed to convert them to more useful dtypes. Fatalities, Aboard and Ground refer to a discrete number, so these were converted to ```int```.\n",
    "\n",
    "```python\n",
    "df['Fatalities'] = pd.to_numeric(df['Fatalities'], downcast=\"integer\", errors=\"coerce\")\n",
    "\n",
    "df['Ground'] = pd.to_numeric(df['Ground'], downcast=\"integer\", errors=\"coerce\")\n",
    "\n",
    "```\n",
    "\n",
    "The most crucial conversion was Time and Date to Datetime objects, as this allowed us to better manipulate and access these data. With a Datetime object, pands gives us specific .dt methods to use: for instance, to extract the Year, to get comparisons/deltas between dates or times, or to return the day of the week that date was on.\n",
    "\n",
    "```python\n",
    "df[\"Time\"] = pd.to_datetime(df[\"Time\"], format='%HH:%MM', errors=\"ignore\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dropping entries and Series\n",
    "\n",
    "At the initial stages of getting to know the dataset and planning our analysis, we identified that the columns *Flight #*, *Route*, *Registration*, *cn/In* weren't useful for what we had proposed to look at. These four columns were dropped. Additionally, three rows (364, 423, 768) for which the entries couldn't be verified were also dropped. This was followed by reseting the index for all the dataframe entries.\n",
    "\n",
    "```python\n",
    "#dropping and reindexing\n",
    "df = dframe.drop([364, 423, 768])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#checking duplicates\n",
    "df_dupes = df.duplicated() #returns a series with bool values\n",
    "df_dupes[(df_dupes == True)] #filter: if it returns an empty list, there are no duplicate rows... happy days!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Data Preparation results\n",
    "\n",
    "The end result was an almost fully complete dataset - Time, Operator and Type are the only incomplete columns -, with no missing values in the columns that we'll base most of our analysis on - Date, Location, Aboard, Fatalities and Ground. The resulting dataset was exported as .csv to keep as a backup record.\n",
    "\n",
    "With this, we can move on to the next step, EDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "\n",
    "### 2.1 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Google Fusion Table\n",
    "\n",
    "The [table](https://www.google.com/fusiontables/DataSource?docid=1vGwRQjKd9R2-yRNynsV9_Dn2ecNYn4YE7mVPsewZ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
